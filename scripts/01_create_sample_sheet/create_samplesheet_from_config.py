#!/usr/bin/env python3
"""
Create nf-core samplesheet rows by querying the OceanOmics PostgreSQL DB, using a single
bash-compatible pipeline config file (KEY=VALUE). Postgres credentials are read from an
INI file whose path is provided in the pipeline config as POSTGRES_CFG.

Example:
  singularity run $SING/psycopg2:0.1.sif python create_samplesheet_from_config.py genomenotes_pipeline.conf

Pipeline config must include at least:
  POSTGRES_CFG=~/postgresql_details/oceanomics.cfg
  OG_IDS="OG38,OG39"
  STAGING_BASE_DIR=/scratch/pawsey0964/{user}/
=
"""
from __future__ import annotations

import os
import sys
import getpass
import configparser
from pathlib import Path
from datetime import date
from typing import Dict, List

import pandas as pd
import psycopg2


def load_kv_config(path: str) -> Dict[str, str]:
    """
    Load a simple KEY=VALUE config file (bash-compatible).
    - Ignores blank lines and lines starting with '#'
    - Strips surrounding quotes from values ("..." or '...')
    """
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"❌ Config file does not exist: {path}")

    cfg: Dict[str, str] = {}
    for raw in p.read_text().splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "=" not in line:
            raise ValueError(f"❌ Invalid config line (expected KEY=VALUE): {raw}")
        k, v = line.split("=", 1)
        k = k.strip()
        v = v.strip()
        if (v.startswith('"') and v.endswith('"')) or (v.startswith("'") and v.endswith("'")):
            v = v[1:-1]
        cfg[k] = v
    return cfg


def require(cfg: Dict[str, str], key: str) -> str:
    if key not in cfg or cfg[key] == "":
        raise ValueError(f"❌ Missing required config key: {key}")
    return cfg[key]


def parse_og_ids(value: str) -> List[str]:
    # Accept "OG1,OG2" or "OG1 OG2"
    value = value.strip().strip(",")
    if not value:
        return []
    parts: List[str] = []
    for chunk in value.replace(",", " ").split():
        c = chunk.strip()
        if c:
            parts.append(c)
    return parts


def expand_user_placeholders(s: str, user: str) -> str:
    return s.replace("{user}", user)


def build_function_sql(staging_base_dir: str) -> str:
    """
    Inject staging_base_dir into the SQL function.
    The base dir should NOT include a trailing slash.
    """
    base = staging_base_dir.rstrip("/")
    base_sql = base.replace("'", "''")  # SQL literal escape

    return f"""
CREATE OR REPLACE FUNCTION build_nfcore_samplesheet_rows(in_og_ids text[])
RETURNS TABLE (
  sample        text,
  hifi_dir      text,
  hic_dir       text,
  assembly      text,
  busco_genes   text,
  bioproject_id text,
  version       text,
  date          text,
  tolid         text,
  taxid         bigint,
  species       text
)
LANGUAGE sql
AS $$
WITH p AS (
  SELECT unnest(in_og_ids) AS og_id
),
latest_seq AS (
  SELECT DISTINCT ON (seq.og_id)
         seq.og_id,
         seq.seq_date::date AS seq_date
  FROM sequencing seq
  JOIN p ON seq.og_id = p.og_id
  WHERE seq.technology = 'PacBio'
  ORDER BY seq.og_id, seq.seq_date DESC
),
smp AS (
  SELECT DISTINCT ON (s.og_id)
         s.og_id,
         s.nominal_species_id,
         s.tol_id,
         s.ncbi_bioproject_id_lvl_3_hifi
  FROM sample s
  JOIN p ON s.og_id = p.og_id
  ORDER BY s.og_id
)
SELECT DISTINCT ON (p.og_id)
  p.og_id AS sample,
  '{base_sql}/'||p.og_id||'/hifi'     AS hifi_dir,
  '{base_sql}/'||p.og_id||'/hic'      AS hic_dir,
  '{base_sql}/'||p.og_id||'/assembly' AS assembly,
  '{base_sql}/'||p.og_id||'/busco'    AS busco_genes,
  smp.ncbi_bioproject_id_lvl_3_hifi AS bioproject_id,
  CASE WHEN rg.og_id IS NOT NULL THEN 'hic2' ELSE 'hic1' END AS version,
  CASE WHEN ls.seq_date IS NOT NULL THEN 'v'||to_char(ls.seq_date,'YYMMDD') END AS date,
  smp.tol_id AS tolid,
  sp.ncbi_taxon_id AS taxid,
  sp.species
FROM p
LEFT JOIN ref_genomes rg ON rg.og_id = p.og_id
LEFT JOIN latest_seq ls  ON ls.og_id = p.og_id
LEFT JOIN smp ON smp.og_id = p.og_id
LEFT JOIN species sp ON sp.species = smp.nominal_species_id
ORDER BY p.og_id;
$$;
""".strip()


def read_postgres_ini(postgres_cfg_path: str) -> Dict[str, str]:
    """
    Read Postgres connection details from an INI file with section [postgres].
    Required keys: dbname, user, password, host
    Optional: port
    """
    postgres_cfg_path = os.path.expanduser(postgres_cfg_path)

    if not os.path.exists(postgres_cfg_path):
        raise FileNotFoundError(f"❌ Postgres config not found: {postgres_cfg_path}")

    pg = configparser.ConfigParser()
    pg.read(postgres_cfg_path)

    if "postgres" not in pg:
        raise ValueError(f"❌ Missing [postgres] section in {postgres_cfg_path}")

    section = pg["postgres"]
    for k in ("dbname", "user", "password", "host"):
        if k not in section or section[k].strip() == "":
            raise ValueError(f"❌ Missing '{k}' in [postgres] section of {postgres_cfg_path}")

    return {
        "dbname": section["dbname"].strip(),
        "user": section["user"].strip(),
        "password": section["password"].strip(),
        "host": section["host"].strip(),
        "port": section.get("port", "5432").strip(),
    }


def main() -> None:
    if len(sys.argv) < 2:
        print("Usage: create_samplesheet_from_config.py <pipeline_config.conf>", file=sys.stderr)
        sys.exit(1)

    conf_path = sys.argv[1]
    cfg = load_kv_config(conf_path)

    user = os.environ.get("USER") or getpass.getuser()
    user = user.replace("'", "").replace("/", "")

    og_ids = parse_og_ids(require(cfg, "OG_IDS"))
    if not og_ids:
        raise ValueError("❌ OG_IDS in config is empty")

    staging_base_dir = expand_user_placeholders(require(cfg, "STAGING_BASE_DIR"), user)

    out_dir = cfg.get("SAMPLESHEET_OUTPUT_DIR", "").strip()
    out_dir = expand_user_placeholders(out_dir, user) if out_dir else ""
    prefix = cfg.get("SAMPLESHEET_FILENAME_PREFIX", "samplesheet").strip() or "samplesheet"

    # --- NEW: read DB connection from POSTGRES_CFG INI file ---
    postgres_cfg = expand_user_placeholders(require(cfg, "POSTGRES_CFG"), user)
    pg = read_postgres_ini(postgres_cfg)

    func_sql = build_function_sql(staging_base_dir)

    conn = None
    cur = None
    try:
        conn = psycopg2.connect(
            dbname=pg["dbname"],
            user=pg["user"],
            password=pg["password"],
            host=pg["host"],
            port=int(pg["port"]),
        )
        cur = conn.cursor()

        # Drop existing function first (can't change OUT columns with CREATE OR REPLACE)
        cur.execute("DROP FUNCTION IF EXISTS build_nfcore_samplesheet_rows(text[]);")
        conn.commit()

        cur.execute(func_sql)
        conn.commit()

        cur.execute("SELECT * FROM build_nfcore_samplesheet_rows(%s);", (og_ids,))
        rows = cur.fetchall()
        cols = [d[0] for d in cur.description]
        df = pd.DataFrame(rows, columns=cols)

        # taxid nullable integer
        if "taxid" in df.columns:
            df["taxid"] = pd.to_numeric(df["taxid"], errors="coerce").astype("Int64")

        missing_rows = df[df.isnull().any(axis=1)]
        if not missing_rows.empty:
            print("\nRows with missing values:\n", file=sys.stderr)
            print(missing_rows.to_string(index=False), file=sys.stderr)

        today = date.today().strftime("%Y%m%d")
        filename = f"{prefix}_{today}.csv"

        if out_dir:
            Path(out_dir).mkdir(parents=True, exist_ok=True)
            out_path = str(Path(out_dir) / filename)
        else:
            out_path = str(Path.cwd() / filename)

        df.to_csv(out_path, index=False)
        print(f"✅ Samplesheet saved to: {out_path}")

    finally:
        if cur is not None:
            cur.close()
        if conn is not None:
            conn.close()


if __name__ == "__main__":
    main()
